---
title: "Shiftkey app"
author: "Yaswanth Pulavarthi"
date: "9/22/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,warning=FALSE, message=FALSE)
```

## Description 

The goal of this project is just to display that you've gotten used to working with the data and that you are on track to create your prediction algorithm. 



```{r loading }

library(dplyr)
library(plyr)
library(quanteda)
library(tm)
library(ggplot2)
library(NLP)
library(data.table)

#Datasets
en_blog <- readLines(con = "C:/Users/Yaswanth Pulavarthi/Documents/final/en_US/en_US.blogs.txt", encoding= "UTF-8", skipNul = T)
en_news <- readLines(con = "C:/Users/Yaswanth Pulavarthi/Documents/final/en_US/en_US.news.txt", encoding= "UTF-8", skipNul = T)
en_twitter <- readLines(con = "C:/Users/Yaswanth Pulavarthi/Documents/final/en_US/en_US.twitter.txt", encoding= "UTF-8", skipNul = T)


```

## Data Sampling
Data is so large to run on PC. And to avoid long time waiting, we sampled out 10 percent of data.


```{r Sample, echo=FALSE}
sample_blog_vector <- sample(length(en_blog), length(en_blog) * 0.1)
sample_news_vector <- sample(length(en_news), length(en_news) * 0.1)
sample_twitter_vector <- sample(length(en_twitter), length(en_twitter) * 0.1)
sample_blog <- en_blog[sample_blog_vector]
sample_news<- en_news[sample_news_vector]
sample_twitter <- en_twitter[sample_twitter_vector]




```



```{r shaping data }
vector <- c(sample_blog, sample_news, sample_twitter)
corpus <- corpus(vector)
```



```{r cleaning }
cleaning <- tokens(
    x = tolower(corpus),
    remove_punct = TRUE,
    remove_twitter = TRUE,
    remove_numbers = TRUE,
    remove_hyphens = TRUE,
    remove_symbols = TRUE,
    remove_url = TRUE
)

```

```{r token}
stem_words <- tokens_wordstem(cleaning, language = "english")

```

```{r dfm }
bi_gram <- tokens_ngrams(stem_words, n = 2)
tri_gram <- tokens_ngrams(stem_words, n = 3)

uni_DFM <- dfm(stem_words)
bi_DFM <- dfm(bi_gram)
tri_DFM <- dfm(tri_gram)

# Let us trim the N-Grams for faster calculations
uni_DFM <- dfm_trim(uni_DFM, 3)
bi_DFM <- dfm_trim(bi_DFM, 3)
tri_DFM <- dfm_trim(tri_DFM, 3)
```

```{r wordcount }
# Create named vectors with counts of words 
num_uni <- colSums(uni_DFM)
num_bi <- colSums(bi_DFM)
num_tri <- colSums(tri_DFM)

# Create data tables with individual words as columns
uni_words <- data.table(One_Word = names(num_uni), Frequency = num_uni)

bi_words <- data.table(
        One_Word = sapply(strsplit(names(num_bi), "_", fixed = TRUE), '[[', 1),
        Two_Words = sapply(strsplit(names(num_bi), "_", fixed = TRUE), '[[', 2),
        Frequency = num_bi)

tri_words <- data.table(
        One_Word = sapply(strsplit(names(num_tri), "_", fixed = TRUE), '[[', 1),
        Two_Words = sapply(strsplit(names(num_tri), "_", fixed = TRUE), '[[', 2),
        Three_Words = sapply(strsplit(names(num_tri), "_", fixed = TRUE), '[[', 3),
        Frequency = num_tri)
```

```{r topwords }
uni_words[order(uni_words$Frequency, decreasing = T), ][1:50]
bi_words[order(bi_words$Frequency, decreasing = T), ][1:50]
tri_words[order(tri_words$Frequency, decreasing = T), ][1:50]
```

```{r tri_plot}
top10tri<-tri_words[order(tri_words$Frequency, decreasing = T), ][1:10]
twocomb <- paste(top10tri$One_Word, top10tri$Two_Words,top10tri$Three_Words)
qplot(twocomb,top10tri$Frequency)->q
q+geom_col(aes(twocomb,top10tri$Frequency))+ggtitle("Top ten tri words")
```

```{r bi_plot}
top10bi<-bi_words[order(bi_words$Frequency, decreasing = T), ][1:10]
twocomb <- paste(top10bi$One_Word, top10bi$Two_Words)
qplot(twocomb,top10bi$Frequency)->q
q+geom_col(aes(twocomb,top10bi$Frequency))+ggtitle("Top ten bi words")
```

```{r uniplot}

qplot(One_Word,Frequency,data=uni_words[order(uni_words$Frequency, decreasing = T), ][1:10])->q
q+geom_col(aes(One_Word,Frequency))+ggtitle("Top ten uni words")

```



